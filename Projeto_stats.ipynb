{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeto 6 - Lufa Lufa ‚ö°ü¶°‚ö°\n",
    "\n",
    "- Bruno Santos Bezerra\n",
    "- Daniel Garcia Ribeiro\n",
    "- Fernanda Defalco\n",
    "- Guilherme Tyszka\n",
    "- Gustavo Perbone\n",
    "- Rodrigo Cruz\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "Q1 - ESTAT√çSTICA DESCRITIVA\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de Dados - Covid-19: https://brasil.io/dataset/covid19/caso_full/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colunas:\n",
    "\n",
    "- city: nome do munic√≠pio (pode estar em branco quando o registro √© referente ao estado, pode ser preenchido com Importados/Indefinidos tamb√©m).\n",
    "- city_ibge_code: c√≥digo IBGE do local.\n",
    "- date: data de coleta dos dados no formato YYYY-MM-DD.\n",
    "- epidemiological_week: n√∫mero da semana epidemiol√≥gica no formato YYYYWW.\n",
    "- estimated_population: popula√ß√£o estimada para esse munic√≠pio/estado em 2020, segundo o IBGE. (acesse o script que faz o download e convers√£o dos dados de popula√ß√£o).\n",
    "- estimated_population_2019: popula√ß√£o estimada para esse munic√≠pio/estado em 2019, segundo o IBGE. ATEN√á√ÉO: essa coluna possui valores desatualizados, prefira usar a coluna estimated_population.\n",
    "- is_last: campo pr√©-computado que diz se esse registro √© o mais novo para esse local, pode ser True ou False (caso filtre por esse campo, use \n",
    "is_last=True ou is_last=False, n√£o use o valor em min√∫sculas).\n",
    "- is_repeated: campo pr√©-computado que diz se as informa√ß√µes nesse registro foram publicadas pela Secretaria Estadual de Sa√∫de no dia date ou se o dado √© repetido do √∫ltimo dia em que o dado est√° dispon√≠vel (igual ou anterior a date). Isso ocorre pois nem todas as secretarias publicam boletins todos os dias. Veja tamb√©m o campo last_available_date.\n",
    "- last_available_confirmed: n√∫mero de casos confirmados do √∫ltimo dia dispon√≠vel igual ou anterior √† data date.\n",
    "- last_available_confirmed_per_100k_inhabitants: n√∫mero de casos confirmados por 100.000 habitantes (baseado em estimated_population) do √∫ltimo dia dispon√≠vel igual ou anterior √† data date.\n",
    "- last_available_date: data da qual o dado se refere.\n",
    "- last_available_death_rate: taxa de mortalidade (mortes / confirmados) do √∫ltimo dia dispon√≠vel igual ou anterior √† data date.\n",
    "- last_available_deaths: n√∫mero de mortes do √∫ltimo dia dispon√≠vel igual ou anterior √† data date.\n",
    "- order_for_place: n√∫mero que identifica a ordem do registro para este local. O registro referente ao primeiro boletim em que esse local aparecer ser√° \n",
    "contabilizado como 1 e os demais boletins incrementar√£o esse valor.\n",
    "- place_type: tipo de local que esse registro descreve, pode ser city ou state.\n",
    "- state: sigla da unidade federativa, exemplo: SP.\n",
    "- new_confirmed: n√∫mero de novos casos confirmados desde o √∫ltimo dia (note que caso is_repeated seja True, esse valor sempre ser√° 0 e que esse valor pode ser negativo caso a SES remaneje os casos desse munic√≠pio para outro). - Delta de varia√ß√£o \n",
    "- new_deaths: n√∫mero de novos √≥bitos desde o √∫ltimo dia (note que caso is_repeated seja True, esse valor sempre ser√° 0 e que esse valor pode ser negativo caso a SES remaneje os casos desse munic√≠pio para outro).  - Delta de varia√ß√£o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "import statistics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"caso_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns= ['estimated_population_2019','is_repeated','order_for_place'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "Descri√ß√£o das Vari√°veis\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                  **Vari√°vel**                 |            **Tipo**            |\n",
    "|:---------------------------------------------:|:------------------------------:|\n",
    "| city                                          |            Qualitativa Nominal |\n",
    "| city_ibge_code                                |            Qualitativa Nominal |\n",
    "| date                                          |            Qualitativa Ordinal |\n",
    "| epidemiological_week                          |            Qualitativa Ordinal |\n",
    "| estimated_population                          |          Quantitativa Discreta |\n",
    "| last_available_confirmed                      |          Quantitativa Discreta |\n",
    "| last_available_confirmed_per_100k_inhabitants |          Quantitativa Cont√≠nua |\n",
    "| last_available_date                           |            Qualitativa Ordinal |\n",
    "| last_available_death_rate                     |          Quantitativa Cont√≠nua |\n",
    "| last_available_deaths                         |          Quantitativa Discreta |\n",
    "| place_type                                    |            Qualitativa Nominal |\n",
    "| state                                         |            Qualitativa Nominal |\n",
    "| new_confirmed                                 |          Quantitativa Discreta |\n",
    "| new_deaths                                    |          Quantitativa Discreta |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'city_ibge_code':object, 'epidemiological_week':object})\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Tratando os nulos\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando os valores nulos da coluna city, pode-se ver que todos s√£o nulos porque representam estados, ou seja, n√£o h√° cidades faltando\n",
    "df[df['city'].isnull()]['place_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando as cidades que tem c√≥digo do IBGE nulo, pode-se ver que todos os nulos s√£o de uma cidade chamada 'Importados/Indefinidos' \n",
    "df[df['city_ibge_code'].isnull()]['city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foi feita a remo√ß√£o desses dados, pois n√£o temos informa√ß√µes das suas caracter√≠sticas\n",
    "df = df.drop(df[df['city']=='Importados/Indefinidos'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para o caso dos casos confirmados por 100k habitantes, percebe-se que esses valores est√£o nulos nos casos que o n√∫mero de confirmados √© zero\n",
    "# df[df['last_available_confirmed_per_100k_inhabitants'].isnull()]\n",
    "df[df['last_available_confirmed_per_100k_inhabitants'].isnull()]['last_available_confirmed'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como o n√∫mero de confirmados √© zero, a raz√£o de confirmados por 100k habitantes tamb√©m √© zero\n",
    "df['last_available_confirmed_per_100k_inhabitants'] = df['last_available_confirmed_per_100k_inhabitants'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restaram s√≥ as cidades, como j√° explicado acima\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um subset s√≥ com as cidades, para melhor an√°lise dos dados de interesse\n",
    "df_city = df.iloc[:,[0,11,12,2,3,4,6,10,7,9,13, 14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df_city.loc[df[\"city\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.rename(columns={\"last_available_confirmed_per_100k_inhabitants\": \"last_confirmed_per_100k\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "M√©tricas de Posi√ß√£o\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitativas Nominais\n",
    "#### city, place_type, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criou-se essa coluna pois existem cidades com mesmo nome em estados diferentes\n",
    "df_city['citystate'] = df_city[\"city\"]+'-'+df_city[\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.loc[:,[\"citystate\",\"place_type\",\"state\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclui-se para city, place_type e state:\n",
    "- Os itens que apareceram com mais frequ√™ncia s√£o S√£o Paulo - SP (762 vezes), city (3.819.883 vezes),MG (573.315 vezes);\n",
    "- citystate representa 5570 cidades diferentes;\n",
    "- place_type confirma que selecionamos apenas cidades (tem apenas 1 valor);\n",
    "- state tem 27 valores diferentes, ou seja, abrange todos os estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_1 = px.histogram(df_city.groupby(['citystate']).size().to_frame().sort_values([0],ascending = False).reset_index().head(50), x = 'citystate', y =0)\n",
    "hist_1.update_xaxes(tickangle=60, categoryorder = 'total ascending')\n",
    "hist_1.update_layout(\n",
    "    xaxis_title=None,\n",
    "    yaxis_title=\"Frequ√™ncia\",\n",
    "    showlegend=False,\n",
    "    title={\n",
    "    'text': 'Top 50 - Cidades com maior frequ√™ncia',\n",
    "    'y':0.93,\n",
    "    'x':0.5,\n",
    "    'font_color': 'black'}\n",
    "    )\n",
    "hist_1.update_traces(marker=dict(color='#25215E'))\n",
    "hist_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_5 = px.histogram(df_city.groupby(['citystate']).size().to_frame().sort_values([0],ascending = False).tail(50).reset_index(), x = 'citystate', y =0)\n",
    "hist_5.update_xaxes(tickangle=60, categoryorder = 'total ascending')\n",
    "hist_5.update_layout(\n",
    "    xaxis_title=None,\n",
    "    yaxis_title=\"Frequ√™ncia\",\n",
    "    showlegend=False,\n",
    "    title={\n",
    "    'text': 'Top 50 - Cidades com menor frequ√™ncia',\n",
    "    'y':0.93,\n",
    "    'x':0.5,\n",
    "    'font_color': 'black'}\n",
    "    )\n",
    "hist_5.update_traces(marker=dict(color='#25215E'))\n",
    "hist_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_2 = px.histogram(df_city,'state')\n",
    "hist_2.update_xaxes(categoryorder = 'total ascending')\n",
    "hist_2.update_layout(\n",
    "    xaxis_title=None,\n",
    "    yaxis_title=\"Frequ√™ncia\",\n",
    "    showlegend=False,\n",
    "    title={\n",
    "    'text': 'Frequ√™ncia dos estados',\n",
    "    'y':0.93,\n",
    "    'x':0.5,\n",
    "    'font_color': 'black'}\n",
    "    )\n",
    "hist_2.update_traces(marker=dict(color='#25215E'))\n",
    "hist_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitativas Ordinais\n",
    "#### date, epidemiological_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.loc[:,[\"date\", \"epidemiological_week\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A vari√°vel *date* cont√©m 762 valores √∫nicos, ou seja, representa aproximadamente 110 semanas diferentes (7 dias em cada).\n",
    "    - O dia mais frequente √© a 02 de Abril de 2021 (5570 apari√ß√µes).\n",
    "\n",
    "#\n",
    "\n",
    "- A vari√°vel *epidemiological_week* cont√©m 110 valores √∫nicos, ou seja, representa 110 semanas diferentes (pouco mais de 2 anos).\n",
    "    - A semana com mais dados √© a 14·µÉ semana de 2021 (202114)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_3 = px.bar(df_city.pivot_table(index='date', values='epidemiological_week', aggfunc='count').sort_values('epidemiological_week', ascending=False))\n",
    "hist_3.update_xaxes(categoryorder = 'total ascending')\n",
    "hist_3.update_layout(\n",
    "    xaxis_title=None,\n",
    "    yaxis_title=\"Frequ√™ncia\",\n",
    "    showlegend=False,\n",
    "    title={\n",
    "    'text': 'Frequ√™ncia das datas',\n",
    "    'y':0.93,\n",
    "    'x':0.5,\n",
    "    'font_color': 'black'}\n",
    "    )\n",
    "hist_3.update_traces(marker=dict(color='#25215E'))\n",
    "hist_3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_4 = px.bar(df_city.pivot_table(index='epidemiological_week', values='date', aggfunc='count').sort_values('date', ascending=False))\n",
    "hist_4.update_xaxes(categoryorder = 'total ascending')\n",
    "hist_4.update_layout(\n",
    "    xaxis_title=None,\n",
    "    yaxis_title=\"Frequ√™ncia\",\n",
    "    showlegend=False,\n",
    "    title={\n",
    "    'text': 'Frequ√™ncia das semanas',\n",
    "    'y':0.93,\n",
    "    'x':0.5,\n",
    "    'font_color': 'black'}\n",
    "    )\n",
    "hist_4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *date*: Observando o gr√°fico acima, podemos afirmar que a moda n√£o √© apenas o dia 02/02/2021. A partir de 03/10/2020 j√° tinhamos 5570 valores reportados por dia\n",
    "#\n",
    "* *epidemiological_week*: Observando o gr√°fico acima, podemos afirmar que a moda n√£o √© apenas a 14·µÉ semana de 2021. A partir da 41·µÉ semana de 2020 j√° vinham sendo reportador 38990 resultados por semana!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitativas Discretas\n",
    "#### estimated_population, last_available_confirmed, last_available_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coluna in [\"estimated_population\", \"last_available_confirmed\",\"last_available_deaths\",]:\n",
    "    media = round(df_city[coluna].mean(), 2)\n",
    "    mediana = round(df_city[coluna].median(),2)\n",
    "    moda = statistics.mode(df_city[coluna])\n",
    "    print(f\" M√©dia de {coluna} - {media}\\n Mediana de {coluna} - {mediana}\\n Moda de {coluna} - {moda}\\n-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.loc[:,[\"estimated_population\", \"last_available_confirmed\",\"last_available_deaths\",]].describe().applymap(lambda x: f\"{x:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A princ√≠pio, pode-se dizer que para as tr√™s vari√°veis, a m√©dia est√° muito acima da mediana, e o coeficiente de varia√ß√£o √© muito maior que um, indicando valores muito altos no conjunto (sabe-se que a m√©dia √© mais sens√≠vel a este tipo de valores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(30,20), sharey=False)\n",
    "fig.tight_layout(pad=10)\n",
    "\n",
    "y  = 0\n",
    "for coluna in ['last_available_deaths', 'last_available_confirmed', \"estimated_population\", \"new_deaths\", \"new_confirmed\"]:\n",
    "    df_city.plot(y=coluna, kind='box', figsize=(10, 5), ax = ax[y]);\n",
    "    y += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pd.Series(df_city.estimated_population.unique()).nlargest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_maiores_cidades = list(pd.Series(df_city.estimated_population.unique()).nlargest())\n",
    "df_city[df_city.estimated_population.isin(lista_maiores_cidades)].sort_values(by='estimated_population', ascending=False).city.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os boxplots indicam muitos outliers. Mas neste caso, apesar de como o gr√°fico se apresenta, n√£o podemos considerar realmente os valores como outliers do conjunto, isso acontece porque h√° muitas cidades dentro do dataset, e elas tem perfis bem diversos\n",
    "Os dois maiores valores em *estimated_population*, por exemplo, representam a cidade de S√£o Paulo e Rio de Janeiro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitativas Cont√≠nuas\n",
    "#### last_confirmed_per_100k, last_available_death_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coluna in [\"last_confirmed_per_100k\", \"last_available_death_rate\"]:\n",
    "    media = round(df_city[coluna].mean(), 2)\n",
    "    mediana = round(df_city[coluna].median(),2)\n",
    "    moda = statistics.mode(df_city[coluna])\n",
    "    print(f\" M√©dia de {coluna} - {media}\\n Mediana de {coluna} - {mediana}\\n Moda de {coluna} - {moda}\\n-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.loc[:,[\"last_confirmed_per_100k\", \"last_available_death_rate\"]].describe().applymap(lambda x: f\"{x:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_1 = df_city.plot(y='last_confirmed_per_100k', kind='box', figsize=(10, 5));\n",
    "box_1.set_xlabel('Confirmados a cada 100k de Habitantes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1,q3 = np.percentile(df_city['last_confirmed_per_100k'], [25,75])\n",
    "iqr_dr = q3-q1\n",
    "lim_sup_dr = q3 +1.5*(iqr_dr)\n",
    "lim_sup_dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O valor m√°ximo dos confirmados por 100 mil habitantes √© 100.000 (100% de mortes) e pelo boxplot pode-se ver que h√° valores superiores a isso\n",
    "df_city.loc[df_city['last_confirmed_per_100k'] > 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultou-se esses casos isolados e percebe-se que s√£o erros\n",
    "df_city[(df_city[\"city\"] == \"N√≠sia Floresta\") & (df_city[\"epidemiological_week\"] == 202147)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz-se a substitui√ß√£o desses valores alterados para o valor correspondente do dia anterior\n",
    "\n",
    "for index,linha in df_city[(df_city[\"last_confirmed_per_100k\"] > 100000)].iterrows():\n",
    "        linha[\"last_confirmed_per_100k\"] = df_city.loc[(df_city[\"city\"] == linha[\"city\"]) & (df_city[\"date\"] == linha[\"date\"] + timedelta(days = 1))][\"last_confirmed_per_100k\"]\n",
    "        linha[\"last_available_confirmed\"] = df_city.loc[(df_city[\"city\"] == linha[\"city\"]) & (df_city[\"date\"] == linha[\"date\"] + timedelta(days = 1))][\"last_available_confirmed\"]\n",
    "        linha[\"last_available_death_rate\"] = df_city.loc[(df_city[\"city\"] == linha[\"city\"]) & (df_city[\"date\"] == linha[\"date\"] + timedelta(days = 1))][\"last_available_death_rate\"]\n",
    "        df_city[\"last_confirmed_per_100k\"][index] = linha[\"last_confirmed_per_100k\"]\n",
    "        df_city[\"last_available_confirmed\"][index] = linha[\"last_available_confirmed\"]\n",
    "        df_city[\"last_available_death_rate\"][index] = linha[\"last_available_death_rate\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_1 = df_city.plot(y='last_confirmed_per_100k', kind='box', figsize=(10, 5));\n",
    "box_1.set_xlabel('Confirmados a cada 100k de Habitantes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando se os valores est√£o coerentes para last_available_death_rate\n",
    "box_2 = df_city.plot(y='last_available_death_rate', kind='box', figsize=(10, 5));\n",
    "box_2.set_xlabel('Taxa de morte (mortes/confirmados)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, o maior valor √© 1, o que significa que todos os confirmados naquela cidade vieram a √≥bito. Vamos verificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.loc[df_city['last_available_death_rate'] ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confirmados: ', df_city.loc[df_city['last_available_death_rate'] ==1].last_available_confirmed.unique())\n",
    "print('√ìbitos: ', df_city.loc[df_city['last_available_death_rate'] ==1].last_available_deaths.unique())\n",
    "print('Datas menor e maior: ', pd.to_datetime(df_city.loc[df_city['last_available_death_rate'] ==1].date.unique().min()).strftime('%d.%B.%Y'), '&', pd.to_datetime(df_city.loc[df_city['last_available_death_rate'] ==1].date.unique().max()).strftime('%d.%B.%Y') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rela√ß√£o final das mortes de covid no Brasil por estado\n",
    "df_state = df[df['city'].isna()]\n",
    "df_state = df_state.drop(columns=['city'])\n",
    "df_final_record_bystate = df_state.loc[df['is_last'] == True]\n",
    "df_final_record_bystate['deaths_per_100k_inhabintants'] = df_final_record_bystate.loc[:, 'last_available_death_rate']*100000\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "sns.barplot(x='state', y='deaths_per_100k_inhabintants', data=df_final_record_bystate)\n",
    "plt.title('Total de mortes por 100 mil habitantes de COVID-19 por estado (atualizado 27/03/2022)', fontsize=18)\n",
    "plt.xlabel(None)\n",
    "plt.ylabel('N√∫mero de mortes')\n",
    "plt.grid(axis='y', alpha=.3)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclui-se que esses dados s√£o corretos. Para uma alta taxa de √≥bitos, isto s√≥ ocorreu quando havia poucas confirma√ß√µes e todas vieram a ser √≥bitos (entre 1 e 3). Essas situa√ß√µes ocorreram no primeiro ano de pandemia, entre mar√ßo de 2020 e mar√ßo de 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Correla√ß√£o\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pearson - Verificar a exist√™ncia de uma correla√ß√£o linear\n",
    "- Spearman - Verificar o crescimento/descrescimento entre as vari√°veis (se uma cresce a outra cresce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson\n",
    "df_city.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "heat_1 = sns.heatmap(df_city.corr(), cmap='RdYlGn', vmin=-1, vmax=1, annot = True, annot_kws={'size':12});\n",
    "heat_1.set_xticklabels(heat_1.get_xmajorticklabels(), fontsize = 12);\n",
    "heat_1.set_yticklabels(heat_1.get_ymajorticklabels(), fontsize = 12);\n",
    "heat_1.tick_params(axis='x', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.scatterplot(x=df_city.last_available_deaths, y=df_city.last_available_confirmed, hue=df_city.state)\n",
    "plt.legend(bbox_to_anchor=(1,1.01));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "df_temp = df_city[df_city['last_available_deaths'] > 7000]\n",
    "sns.scatterplot(x=df_temp.last_available_deaths, y=df_temp.last_available_confirmed, hue=df_temp.state)\n",
    "plt.legend(bbox_to_anchor=(1,1.01));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "heat_1 = sns.heatmap(df_city.corr(method = \"spearman\"), cmap='RdYlGn', vmin=-1, vmax=1, annot = True, annot_kws={'size':12});\n",
    "heat_1.set_xticklabels(heat_1.get_xmajorticklabels(), fontsize = 12);\n",
    "heat_1.set_yticklabels(heat_1.get_ymajorticklabels(), fontsize = 12);\n",
    "heat_1.tick_params(axis='x', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "disp_1 = sns.scatterplot(data = df_city, y = 'last_available_confirmed', x = 'last_confirmed_per_100k', color = '#353942');\n",
    "disp_1.set_ylabel('Casos Confirmados', fontsize = 15);\n",
    "disp_1.set_xlabel('Confirmados a cada 100 mil Habitantes', fontsize = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "disp_1 = sns.scatterplot(data = df_city[df_city[\"city\"] == 'S√£o Paulo'], y = 'last_available_confirmed', x = 'last_confirmed_per_100k', color = '#353942');\n",
    "disp_1.set_ylabel('Casos Confirmados', fontsize = 15);\n",
    "disp_1.set_xlabel('Confirmados a cada 100 mil Habitantes', fontsize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No geral, olhando todas as cidades, a rela√ß√£o n√£o √© linear (Pearson de 0.13), pois cada cidade tem a sua pr√≥pria din√¢mica de cont√°gio e a a densidade populacional tamb√©m √© bem diferente. J√°, filtrando uma cidade, v√™-se que a rela√ß√£o √© linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "Q2 - PROBABILIDADE E INFER√äNCIA / TESTE DE HIP√ìTESE\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('values_squad1.npy')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_num = 200 # n√∫mero de bins a ser considerado\n",
    "interval = np.linspace(np.min(data), np.max(data), bins_num)\n",
    "plt.hist(data, bins=interval);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = np.var(data)\n",
    "s = np.std(data)\n",
    "x = np.mean(data)\n",
    "values, bins_val = np.histogram(data, bins = interval)\n",
    "print('M√©dia: ', x, ' Desvio: ', s, ' Vari√¢ncia: ', s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste da distribui√ß√£o Normal:\n",
    "\n",
    "$\\left\\{\\begin{array}{l} H_0:\\ O_i = E_i \\\\ H_1:\\ O_i \\neq E_i  \\end{array}\\right.$\n",
    "\n",
    "Considere:\n",
    "$\\chi^2$ = $\\sum_{i=1}^{n}$ $\\frac{(O_{i} - E_{i})^2}{(E_{i})}$.\n",
    "\n",
    "Rejeita-se a hip√≥tese nula se: $\\chi^2$ > $\\chi^2_{\\alpha,\\ n-p-1}$, onde $\\alpha$ √© o n√≠vel de signific√¢ncia, $n$ √© o n√∫mero de intervalos observados (bins) e $p$ √© o n√∫mero de par√¢metros estimados na distribui√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # n√≠vel de signfic√¢ncia do teste\n",
    "\n",
    "# distribui√ß√£o normal te√≥rica dos dados: \n",
    "dist_normal = stats.norm(loc=x, scale=s)\n",
    "norm_cdf = dist_normal.cdf(interval)\n",
    "area_bins_norm = norm_cdf[1:] - norm_cdf[:-1]\n",
    "frequencia_teorica_norm = area_bins_norm * data.shape[0]\n",
    "\n",
    "# teste:\n",
    "chi2_norm = 0\n",
    "for i in range(len(frequencia_teorica_norm)):\n",
    "    if frequencia_teorica_norm[i] > 0.00001:\n",
    "        chi2_norm += ((frequencia_teorica_norm[i] - values[i])**2/frequencia_teorica_norm[i])\n",
    "    else:\n",
    "        chi2_norm += 0\n",
    "chi2_teste_norm = stats.chi2.ppf(alpha, df=bins_num-2-1) # = 239.8774290531936; bins=200\n",
    "if chi2_norm < chi2_teste_norm  :\n",
    "  print(f'Hip√≥tese nula N√ÉO rejeitada pois chi2 ({chi2_norm:.2f}) < chi2_teste ({chi2_teste_norm:.2f})')\n",
    "else:\n",
    "  print(f'Hip√≥tese nula rejeitada pois chi2 ({chi2_norm:.2f}) > chi2_teste ({chi2_teste_norm:.2f})')\n",
    "\n",
    "# gr√°fico:\n",
    "plt.plot(bins_val[:-1], frequencia_teorica_norm, label='Curva Te√≥rica')\n",
    "plt.plot(bins_val[:-1], values, label='Curva Observada')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste da distribui√ß√£o Gamma:\n",
    "\n",
    "$\\left\\{\\begin{array}{l} H_0:\\ O_i = E_i \\\\ H_1:\\ O_i \\neq E_i  \\end{array}\\right.$\n",
    "\n",
    "Considere:\n",
    "$\\chi^2$ = $\\sum_{i=1}^{n}$ $\\frac{(O_{i} - E_{i})^2}{(E_{i})}$.\n",
    "\n",
    "Rejeita-se a hip√≥tese nula se: $\\chi^2$ > $\\chi^2_{\\alpha,\\ n-p-1}$, onde $\\alpha$ √© o n√≠vel de signific√¢ncia, $n$ √© o n√∫mero de intervalos observados (bins) e $p$ √© o n√∫mero de par√¢metros estimados na distribui√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_shape, fit_loc, fit_scale = stats.gamma.fit(data)\n",
    "dist_gamma = stats.gamma(a=fit_shape, loc=fit_loc, scale=fit_scale)\n",
    "gamma_cdf = dist_gamma.cdf(interval)\n",
    "area_bins_gamma = gamma_cdf[1:] - gamma_cdf[:-1]\n",
    "frequencia_teorica_gamma = area_bins_gamma * data.shape[0]\n",
    "\n",
    "# teste:\n",
    "chi2_gamma = np.sum((values - frequencia_teorica_gamma)**2/frequencia_teorica_gamma) # = 212.6268094321364; bins=200\n",
    "chi2_teste_gamma = stats.chi2.ppf(alpha, df=bins_num-2-1) # = 239.8774290531936; bins=200\n",
    "if chi2_gamma < chi2_teste_gamma  :\n",
    "  print(f'Hip√≥tese nula N√ÉO rejeitada pois chi2 ({chi2_gamma:.2f}) < chi2_teste ({chi2_teste_gamma:.2f})')\n",
    "else:\n",
    "  print(f'Hip√≥tese nula rejeitada pois chi2 ({chi2_gamma:.2f}) > chi2_teste ({chi2_teste_gamma:.2f})')\n",
    "\n",
    "plt.plot(bins_val[:-1], frequencia_teorica_norm, label='Curva Te√≥rica')\n",
    "plt.plot(bins_val[:-1], values, label='Curva Observada')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste da distribui√ß√£o LogNormal:\n",
    "\n",
    "$\\left\\{\\begin{array}{l} H_0:\\ O_i = E_i \\\\ H_1:\\ O_i \\neq E_i  \\end{array}\\right.$\n",
    "\n",
    "Considere:\n",
    "$\\chi^2$ = $\\sum_{i=1}^{n}$ $\\frac{(O_{i} - E_{i})^2}{(E_{i})}$.\n",
    "\n",
    "Rejeita-se a hip√≥tese nula se: $\\chi^2$ > $\\chi^2_{\\alpha,\\ n-p-1}$, onde $\\alpha$ √© o n√≠vel de signific√¢ncia, $n$ √© o n√∫mero de intervalos observados (bins) e $p$ √© o n√∫mero de par√¢metros estimados na distribui√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√¢metros de entrada stats.lognorm(s, loc, scale) explicados:\n",
    "# loc - No equivalent, this gets subtracted from your data so that 0 becomes the infimum of the range of the data.\n",
    "# scale - exp Œº, where Œº is the mean of the log of the variate. (When fitting, typically you'd use the sample mean of the log of the data.)\n",
    "# s -> shape - the standard deviation of the log of the variate.\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html\n",
    "# https://stackoverflow.com/questions/8870982/how-do-i-get-a-lognormal-distribution-in-python-with-mu-and-sigma?noredirect=1&lq=1\n",
    "\n",
    "alpha = 0.95 # n√≠vel de signfic√¢ncia do teste\n",
    "x_log = np.mean(np.log(data))  # m√©dia do logaritmo dos dados\n",
    "s_log = np.std(np.log(data))  # desvio padr√£o do logaritmo dos dados\n",
    "\n",
    "# distribui√ß√£o lognormal te√≥rica dos dados: \n",
    "dist_log_normal = stats.lognorm(s=s_log, loc=x_log, scale=np.exp(x_log))\n",
    "log_cdf = dist_log_normal.cdf(interval)\n",
    "area_bins = log_cdf[1:] - log_cdf[:-1]\n",
    "frequencia_teorica = area_bins * data.shape[0]\n",
    "\n",
    "# teste:\n",
    "chi2_log = np.sum((values - frequencia_teorica)**2/frequencia_teorica) # = 212.6268094321364; bins=200\n",
    "chi2_teste_log = stats.chi2.ppf(alpha, df=bins_num-2-1) # = 239.8774290531936; bins=200\n",
    "if chi2_log < chi2_teste_log  :\n",
    "  print(f'Hip√≥tese nula N√ÉO rejeitada pois chi2 ({chi2_log:.2f}) < chi2_teste ({chi2_teste_log:.2f})')\n",
    "else:\n",
    "  print(f'Hip√≥tese nula rejeitada pois chi2 ({chi2_log:.2f}) > chi2_teste ({chi2_teste_log:.2f})')\n",
    "\n",
    "# gr√°fico:\n",
    "plt.plot(bins_val[:-1], frequencia_teorica_norm, label='Curva Te√≥rica')\n",
    "plt.plot(bins_val[:-1], values, label='Curva Observada')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Fitter(data,distributions=['gamma','lognorm',\"norm\", \"expon\"])\n",
    "f.fit()\n",
    "f.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.get_best(method = 'sumsquare_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dessa forma, chega-se √† conclus√£o que a distribui√ß√£o que melhor se adapta aos dados fornecidos √© a lognormal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA: Fun√ß√£o que testa v√°rias distribui√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def plots(frequencia_teorica, histogram, bins_val, kind):\n",
    "    plt.plot(bins_val[:-1], frequencia_teorica, label='Curva Te√≥rica')\n",
    "    plt.plot(bins_val[:-1], histogram, label='Curva Observada')\n",
    "    plt.legend()\n",
    "    plt.title(f'{kind} fitting')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def normal_test(data, histogram, bins_val, bins_num, alpha, show_hist=False):\n",
    "    fit_loc, fit_scale = stats.norm.fit(data)\n",
    "    dist = stats.norm(loc=fit_loc, scale=fit_scale)\n",
    "    values = dist.cdf(bins_val)\n",
    "    area_bins = values[1:] - values[:-1]\n",
    "    frequencia_teorica = area_bins * data.shape[0]\n",
    "    chi2 =  np.sum((histogram - frequencia_teorica)**2/frequencia_teorica, where=frequencia_teorica!=0)\n",
    "    chi2_teste = stats.chi2.ppf(1-alpha, df=bins_num-2-1)\n",
    "    if show_hist:\n",
    "        plots(frequencia_teorica, histogram, bins_val, 'Normal')\n",
    "    return f'O teste para a distribui√ß√£o Normal {\"N√ÉO \" if chi2 < chi2_teste else \"\"}rejeita a hip√≥tese nula, chi = {chi2:.2f}, chi_teste = {chi2_teste:.2f}.'\n",
    "\n",
    "def chi2_test(data, histogram, bins_val, bins_num, alpha, show_hist=False):\n",
    "    fit_shape, fit_loc, fit_scale = stats.chi2.fit(data)\n",
    "    dist = stats.chi2(df=fit_shape, loc=fit_loc, scale=fit_scale)\n",
    "    values = dist.cdf(bins_val)\n",
    "    area_bins = values[1:] - values[:-1]\n",
    "    frequencia_teorica = area_bins * data.shape[0]\n",
    "    chi2 =  np.sum((histogram - frequencia_teorica)**2/frequencia_teorica, where=frequencia_teorica!=0)\n",
    "    chi2_teste = stats.chi2.ppf(1-alpha, df=bins_num-2-1)\n",
    "    if show_hist:\n",
    "        plots(frequencia_teorica, histogram, bins_val, 'ChiQuadrado')   \n",
    "    return f'O teste para a distribui√ß√£o ChiQuadrado {\"N√ÉO \" if chi2 < chi2_teste else \"\"}rejeita a hip√≥tese nula, chi = {chi2:.2f}, chi_teste = {chi2_teste:.2f}.'\n",
    "\n",
    "def gamma_test(data, histogram, bins_val, bins_num, alpha, show_hist=False):\n",
    "    fit_shape, fit_loc, fit_scale = stats.gamma.fit(data)\n",
    "    dist = stats.gamma(a=fit_shape, loc=fit_loc, scale=fit_scale)\n",
    "    values = dist.cdf(bins_val)\n",
    "    area_bins = values[1:] - values[:-1]\n",
    "    frequencia_teorica = area_bins * data.shape[0]\n",
    "    chi2 =  np.sum((histogram - frequencia_teorica)**2/frequencia_teorica, where=frequencia_teorica!=0)\n",
    "    chi2_teste = stats.chi2.ppf(1-alpha, df=bins_num-2-1) \n",
    "    if show_hist:\n",
    "        plots(frequencia_teorica, histogram, bins_val, 'Gamma')\n",
    "    return f'O teste para a distribui√ß√£o Gamma {\"N√ÉO \" if chi2 < chi2_teste else \"\"}rejeita a hip√≥tese nula, chi = {chi2:.2f}, chi_teste = {chi2_teste:.2f}.'\n",
    "\n",
    "def log_normal_test(data, histogram, bins_val, bins_num, alpha, show_hist=False):\n",
    "    fit_shape, fit_loc, fit_scale = stats.lognorm.fit(data)\n",
    "    dist = stats.lognorm(s=fit_shape, loc=fit_loc, scale=fit_scale)\n",
    "    values = dist.cdf(bins_val)\n",
    "    area_bins = values[1:] - values[:-1]\n",
    "    frequencia_teorica = area_bins * data.shape[0]\n",
    "    chi2 =  np.sum((histogram - frequencia_teorica)**2/frequencia_teorica, where=frequencia_teorica!=0)\n",
    "    chi2_teste = stats.chi2.ppf(1-alpha, df=bins_num-2-1) \n",
    "    if show_hist:\n",
    "        plots(frequencia_teorica, histogram, bins_val, 'LogNormal')\n",
    "    return f'O teste para a distribui√ß√£o LogNormal {\"N√ÉO \" if chi2 < chi2_teste else \"\"}rejeita a hip√≥tese nula, chi = {chi2:.2f}, chi_teste = {chi2_teste:.2f}.'\n",
    "\n",
    "def weibull_test(data, histogram, bins_val, bins_num, alpha, show_hist=False):\n",
    "    fit_shape, fit_loc, fit_scale = stats.weibull_min.fit(data)\n",
    "    dist = stats.weibull_min(c=fit_shape, loc=fit_loc, scale=fit_scale)\n",
    "    values = dist.cdf(bins_val)\n",
    "    area_bins = values[1:] - values[:-1]\n",
    "    frequencia_teorica = area_bins * data.shape[0]\n",
    "    chi2 =  np.sum((histogram - frequencia_teorica)**2/frequencia_teorica, where=frequencia_teorica!=0)\n",
    "    chi2_teste = stats.chi2.ppf(1-alpha, df=bins_num-2-1) \n",
    "    if show_hist:\n",
    "        plots(frequencia_teorica, histogram, bins_val, 'Weibull')\n",
    "    return f'O teste para a distribui√ß√£o weibull_min {\"N√ÉO \" if chi2 < chi2_teste else \"\"}rejeita a hip√≥tese nula, chi = {chi2:.2f}, chi_teste = {chi2_teste:.2f}.'\n",
    "\n",
    "def goodness_of_fit(data, bins_num, alpha, show_hist=False):\n",
    "    distributions = [normal_test, log_normal_test, gamma_test, chi2_test, weibull_test]\n",
    "    histogram, bins_val = np.histogram(data, bins = bins_num)\n",
    "    for dist in distributions:\n",
    "        print(dist(data, histogram, bins_val, bins_num=bins_num, alpha=alpha, show_hist=show_hist))\n",
    "\n",
    "\n",
    "data = np.load('values_squad1.npy')\n",
    "goodness_of_fit(data=data, bins_num=15, alpha=0.05, show_hist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "Q3 - SERIES TEMPORAIS\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energia = pd.read_csv('ipeadata_consumo_energia.csv', sep=';')\n",
    "df_energia = df_energia.drop(columns='Unnamed: 2')\n",
    "df_energia = df_energia.rename(columns={'Energia el√©trica - consumo - resid√™ncia - quantidade - GWh - Eletrobras - ELETRO12_CEERES12':'Consumo'})\n",
    "df_energia['Consumo'] = df_energia['Consumo'].apply(lambda x: x.replace(',','.'))\n",
    "df_energia['Data'] = df_energia['Data'].apply(lambda x: str(x).replace('.','-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energia['Data'] = pd.to_datetime(df_energia['Data'], format='%Y-%m')\n",
    "df_energia['Consumo'] = df_energia['Consumo'].astype('float')\n",
    "\n",
    "rng = pd.date_range(start = '1976-01-01', end = '2022-07-01', freq='MS')\n",
    "rng = pd.DataFrame(rng)\n",
    "df_energia.loc[:,['Data']] = [rng]\n",
    "\n",
    "df_energia = df_energia.set_index('Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = df_energia['Consumo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(series, model='additive')\n",
    "fig = result.plot()\n",
    "fig.set_size_inches((15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando o gr√°fico acima, podemos observar que o consumo de energia el√©trica em GWh no Brasil vem crescendo ao longo dos anos, o que √© ressaltado pelo gr√°fico da tend√™ncia. Tamb√©m √© poss√≠vel observar a presen√ßa de uma sazonalidade com amplitude por volta de 280 GWh. Al√©m disso, vemos que h√° cinco per√≠odos de sazonaliodades completos em um per√≠odo de 5 anos (repare no gr√°fico de sazonalidade de 1985 a 1990, por exemplo), logo h√° uma sazonalidade anual. Por fim, v√™-se que os res√≠duos se comportam de maneira similar, excetuando per√≠odos com maiores oscila√ß√µes, provavelmente em raz√£o de motivos de infraestrutura, macro e microecon√¥micos. √â interessante ver que os per√≠odos com maiores res√≠duos s√£o aqueles com maiores oscila√ß√µes de consumo, o que, no caso, √© um resultado direto da oscila√ß√£o, ent√£o seria esperado.\\\n",
    "Os per√≠odos com maiores oscila√ß√µes, citados acima, seriam os per√≠odos at√≠picos perguntados no enunciado. Especificamente na √©poca de 2001-2002, ocorreu um racionamento de energia no Brasil, conhecido como apag√£o, em raz√£o da falta de chuvas. Dessa forma, o governo orientou tal racionamento para que n√£o ocorresse outro apag√£o total de energia como se passou em alguns dias daqueles anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_max = df_energia['Consumo'].max()\n",
    "periodo_max = df_energia[df_energia['Consumo'] == valor_max].index\n",
    "\n",
    "valor_min = df_energia['Consumo'].min()\n",
    "periodo_min = df_energia[df_energia['Consumo'] == valor_min].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'O consumo m√°ximo foi de {valor_max} GWh e ocorreu em {str(periodo_max[0])[0:7]}. \\\n",
    "J√° o consumo m√≠nimo foi de {valor_min} GWh e ocorreu em {str(periodo_min[0])[0:7]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vejamos com mais detalhes um per√≠odo de 3 anos para ver a sazonalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_inicial_corte = 2015\n",
    "ano_final_corte = 2018\n",
    "series2 = df_energia['Consumo'][12*(ano_inicial_corte-1976):12*(ano_final_corte-1976)+12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = seasonal_decompose(series2, model='additive')\n",
    "fig = result2.plot()\n",
    "fig.set_size_inches((15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os gr√°ficos acima, pode-se observar que o pico de consumo de energia ocorre sempre no m√™s de janeiro daquele ano, o que coincide com o ver√£o, possivelmente um dos meses mais quentes do ano e que tamb√©m possui diversos eventos associados que podem aumentar o consumo de energia, tal como festas de fim de ano. Al√©m disso, o vale de consumo ocorre em julho, m√™s de inverno, quando n√£o √© necess√°rio utilizar diversos aparelhos dom√©sticos que consomem energia como ar-condicionado.\n",
    "\n",
    "Agora, vejamos o consumo de energia no per√≠odo de 2000 a 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_inicial_corte = 2000\n",
    "ano_final_corte = 2002\n",
    "series3 = df_energia['Consumo'][12*(ano_inicial_corte-1976):12*(ano_final_corte-1976)+12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = seasonal_decompose(series3, model='additive')\n",
    "fig = result3.plot()\n",
    "fig.set_size_inches((15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se observar o efeito do apag√£o de 2001 por volta do m√™s de maio desse ano, que fez com que o consumo ca√≠sse de 7000 GWh a 5000 GWh em pouco tempo. Esse fato foi t√£o significativo que faz com que a sazonalidade do consumo seja alterada quando analisa-se uma janela temporal focada nesse evento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Res√≠duos da s√©rie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = result.resid\n",
    "residual= residual.dropna()\n",
    "plt.plot(residual, label='Residuals');\n",
    "\n",
    "plt.figure()\n",
    "bins_num = 10\n",
    "plt.hist(residual, bins = bins_num);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = np.array(residual)\n",
    "\n",
    "x = np.mean(residual)\n",
    "s = np.std(residual)\n",
    "interval = np.linspace(np.min(residual), np.max(residual), bins_num)\n",
    "values, bins_val = np.histogram(residual, bins = interval)\n",
    "\n",
    "alpha = 0.05 # n√≠vel de signfic√¢ncia do teste\n",
    "\n",
    "# distribui√ß√£o normal te√≥rica dos dados: \n",
    "dist_normal = stats.norm(loc=x, scale=s)\n",
    "norm_cdf = dist_normal.cdf(interval)\n",
    "area_bins_norm = norm_cdf[1:] - norm_cdf[:-1]\n",
    "frequencia_teorica_norm = area_bins_norm * residual.shape[0]\n",
    "\n",
    "# teste:\n",
    "chi2_norm = 0\n",
    "for i in range(len(frequencia_teorica_norm)):\n",
    "    if frequencia_teorica_norm[i] > 0.00001:\n",
    "        chi2_norm += ((frequencia_teorica_norm[i] - values[i])**2/frequencia_teorica_norm[i])\n",
    "    else:\n",
    "        chi2_norm += 0\n",
    "chi2_teste_norm = stats.chi2.ppf(1-alpha, df=bins_num-2-1) # = 239.8774290531936; bins=200\n",
    "if chi2_norm < chi2_teste_norm  :\n",
    "  print(f'Hip√≥tese nula N√ÉO rejeitada pois chi2 ({chi2_norm:.2f}) < chi2_teste ({chi2_teste_norm:.2f})')\n",
    "else:\n",
    "  print(f'Hip√≥tese nula rejeitada pois chi2 ({chi2_norm:.2f}) > chi2_teste ({chi2_teste_norm:.2f})')\n",
    "\n",
    "# gr√°fico:\n",
    "plt.plot(frequencia_teorica_norm, label='Curva Te√≥rica')\n",
    "plt.plot(values, label='Curva Observada')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelo teste acima, n√£o foi poss√≠vel aceitar a hip√≥tese que a distribui√ß√£o dos res√≠duos √© normal. Podemos testar se os eventos que causaram grandes oscila√ß√µes no consumo de energia est√£o influenciando nessa n√£o normalidade dos resultados. Assim, o teste de hip√≥tese a seguir √© feito de 1976 a 1996."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = result.resid\n",
    "residual= residual[0:12*21].dropna()\n",
    "plt.plot(residual, label='Residuals');\n",
    "\n",
    "bins_num = 10\n",
    "plt.figure()\n",
    "plt.hist(residual, bins = bins_num);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = np.array(residual)\n",
    "\n",
    "x = np.mean(residual)\n",
    "s = np.std(residual)\n",
    "interval = np.linspace(np.min(residual), np.max(residual), bins_num)\n",
    "values, bins_val = np.histogram(residual, bins = interval)\n",
    "\n",
    "alpha = 0.05 # n√≠vel de signfic√¢ncia do teste\n",
    "\n",
    "# distribui√ß√£o normal te√≥rica dos dados: \n",
    "dist_normal = stats.norm(loc=x, scale=s)\n",
    "norm_cdf = dist_normal.cdf(interval)\n",
    "area_bins_norm = norm_cdf[1:] - norm_cdf[:-1]\n",
    "frequencia_teorica_norm = area_bins_norm * residual.shape[0]\n",
    "\n",
    "# teste:\n",
    "chi2_norm = 0\n",
    "for i in range(len(frequencia_teorica_norm)):\n",
    "    if frequencia_teorica_norm[i] > 0.00001:\n",
    "        chi2_norm += ((frequencia_teorica_norm[i] - values[i])**2/frequencia_teorica_norm[i])\n",
    "    else:\n",
    "        chi2_norm += 0\n",
    "chi2_teste_norm = stats.chi2.ppf(1-alpha, df=bins_num-2-1) # = 239.8774290531936; bins=200\n",
    "if chi2_norm < chi2_teste_norm  :\n",
    "  print(f'Hip√≥tese nula N√ÉO rejeitada pois chi2 ({chi2_norm:.2f}) < chi2_teste ({chi2_teste_norm:.2f})')\n",
    "else:\n",
    "  print(f'Hip√≥tese nula rejeitada pois chi2 ({chi2_norm:.2f}) > chi2_teste ({chi2_teste_norm:.2f})')\n",
    "\n",
    "# gr√°fico:\n",
    "plt.plot(frequencia_teorica_norm, label='Curva Te√≥rica')\n",
    "plt.plot(values, label='Curva Observada')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, para as condi√ß√µes analisadas, temos que n√£o podemos rejeitar a hip√≥tese nula, indicando que os devios tendem a uma normalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodness_of_fit(data=residual, bins_num=9, alpha=0.05, show_hist=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19db897e25db448848a50bf90fa156adabeba0f0a523309441f8954a7d58fca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
